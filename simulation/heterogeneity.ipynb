{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file is a WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_speeds = [1.0, 0.5, 0.25, 1.5, 0.75, 0.49]  # Simulated speed factors\n",
    "client_loaders = ['a', 'b', 'c', 'd', 'e', 'f']  # Simulated client names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function needs to be made functional\n",
    "\n",
    "def train_client(model, dataloader, speed_factor, epochs=1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Simulate slower/faster training based on speed_factor\n",
    "    for _ in range(int(epochs * speed_factor)):\n",
    "        pass  # Simulated work\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Client trained in {end_time - start_time:.2f} seconds\")\n",
    "    return model.state_dict()  # Return model state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0.25, 'c'), (0.49, 'f')],\n",
       " [(0.5, 'b'), (0.75, 'e')],\n",
       " [(1.0, 'a')],\n",
       " [(1.5, 'd')]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort clients into batches\n",
    "\n",
    "batches = []\n",
    "\n",
    "# sort the clients by speed\n",
    "sorted_clients = sorted(zip(client_speeds, client_loaders), key=lambda x: x[0])\n",
    "\n",
    "# Group clients in windows of 0.5 speed factor\n",
    "for speed, group in groupby(sorted_clients, key=lambda x: x[0] // 0.5):  # Group by speed range\n",
    "    batches.append(list(group))\n",
    "\n",
    "batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.25, 'c'), (0.49, 'f')]\n",
      "[(0.5, 'b'), (0.75, 'e')]\n",
      "[(1.0, 'a')]\n",
      "[(1.5, 'd')]\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    print(batch)\n",
    "    # client_states = []\n",
    "    # for _, client_loader in batch:\n",
    "    #     client_state = train_client(global_model, client_loader, epochs=1)\n",
    "    #     client_states.append(client_state)\n",
    "    \n",
    "    # # Aggregate updates for the current batch\n",
    "    # new_global_state = federated_averaging(global_model, client_states)\n",
    "    # global_model.load_state_dict(new_global_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

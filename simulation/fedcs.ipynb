{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Get the absolute path of the src directory\n",
    "src_path = os.path.abspath('../src')\n",
    "\n",
    "# Add src_path to sys.path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "    \n",
    "import fl\n",
    "from fl import client_selection_with_constraints, select_indices\n",
    "from models import QuantStubModel,calculate_model_size\n",
    "from quantization import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hp = {\n",
    "    \"run_id\": \"test\",\n",
    "    \"run_number\": \"qunatized_mqat_32168_noniid\",\n",
    "    \"learning_rate\": 1e-1,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_clients\": 20,\n",
    "    \"num_rounds\": 10,\n",
    "    \"num_classes\": 10,  # 10 for MNIST\n",
    "    \"classes_per_client\": 1,\n",
    "    \"epochs\": 3,    # number of epochs to train in each round\n",
    "    \"split\": \"NONIID\",   # [\"RANDOM\", \"NONIID\"]\n",
    "    \"quantize\": False,\n",
    "    \"averaging_setting\": \"standard\",    # [\"standard\", \"scalar\", \"kure\", \"mqat\"]\n",
    "    \"lambda_kure\": 1e-4,\n",
    "    \"delta\": 1e-2,\n",
    "    \"bit_widths\": [32],\n",
    "    \"shared_fraction\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_fraction = hp['shared_fraction']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 45600, Train Shared dataset size: 2400, Validation dataset size: 12000, Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load MNIST dataset\n",
    "mnist_full = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"../data\", train=False, transform=transform, download=True)\n",
    "\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(mnist_full))  # 80% for training\n",
    "val_size = len(mnist_full) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(mnist_full, [train_size, val_size])\n",
    "\n",
    "shared_size = int(len(train_dataset) * shared_fraction)\n",
    "remaining_size = len(train_dataset) - shared_size\n",
    "train_shared_dataset, train_dataset = random_split(train_dataset, [shared_size, remaining_size])\n",
    "\n",
    "# Extract indices from Subset objects\n",
    "train_indices = train_dataset.indices  # List of training indices\n",
    "train_shared_indices = train_shared_dataset.indices  # List of training indices\n",
    "val_indices = val_dataset.indices      # List of validation indices\n",
    "\n",
    "# Create training and validation MNIST datasets\n",
    "mnist_train = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=False)\n",
    "mnist_train_shared = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=False)\n",
    "mnist_val = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=False)\n",
    "\n",
    "# Filter datasets by indices\n",
    "mnist_train.data = mnist_train.data[torch.tensor(train_indices)]\n",
    "mnist_train.targets = mnist_train.targets[torch.tensor(train_indices)]\n",
    "mnist_train_shared.data = mnist_train_shared.data[torch.tensor(train_shared_indices)]\n",
    "mnist_train_shared.targets = mnist_train_shared.targets[torch.tensor(train_shared_indices)]\n",
    "mnist_val.data = mnist_val.data[torch.tensor(val_indices)]\n",
    "mnist_val.targets = mnist_val.targets[torch.tensor(val_indices)]\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(mnist_train)}, Train Shared dataset size: {len(mnist_train_shared)}, Validation dataset size: {len(mnist_val)}, Test dataset size: {len(mnist_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsys.path.append(os.path.abspath(\\'../datasets/non-iid-dataset-for-personalized-federated-learning\\'))\\n\\nfrom dataset.mnist_noniid import get_dataset_mnist_extr_noniid\\nfrom collections import Counter\\n\\nnum_users_mnist = 10\\nnclass_mnist = 1\\nnsamples_mnist = 10\\nrate_unbalance_mnist = 1.0\\n\\ntrain_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist = get_dataset_mnist_extr_noniid(num_users_mnist, nclass_mnist, nsamples_mnist, rate_unbalance_mnist)\\nprint(len(user_groups_test_mnist[0]))\\n\\nuser_groups_train_mnist = {\\n    key: [int(idx) for idx in indices]\\n    for key, indices in user_groups_train_mnist.items()\\n}\\n\\n\\nclient_datasets = [\\n    Subset(train_dataset_mnist, indices) for indices in user_groups_train_mnist.values()\\n]\\n\\nfor i, client_dataset in enumerate(client_datasets):\\n    client_loader = torch.utils.data.DataLoader(client_dataset, batch_size=len(client_dataset))\\n    client_samples, client_labels = next(iter(client_loader))\\n    label_counts = Counter(client_labels.tolist())\\n    print(f\"Client {i} label distribution: {dict(label_counts)}\")\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sys.path.append(os.path.abspath('../datasets/non-iid-dataset-for-personalized-federated-learning'))\n",
    "\n",
    "from dataset.mnist_noniid import get_dataset_mnist_extr_noniid\n",
    "from collections import Counter\n",
    "\n",
    "num_users_mnist = 10\n",
    "nclass_mnist = 1\n",
    "nsamples_mnist = 10\n",
    "rate_unbalance_mnist = 1.0\n",
    "\n",
    "train_dataset_mnist, test_dataset_mnist, user_groups_train_mnist, user_groups_test_mnist = get_dataset_mnist_extr_noniid(num_users_mnist, nclass_mnist, nsamples_mnist, rate_unbalance_mnist)\n",
    "print(len(user_groups_test_mnist[0]))\n",
    "\n",
    "user_groups_train_mnist = {\n",
    "    key: [int(idx) for idx in indices]\n",
    "    for key, indices in user_groups_train_mnist.items()\n",
    "}\n",
    "\n",
    "\n",
    "client_datasets = [\n",
    "    Subset(train_dataset_mnist, indices) for indices in user_groups_train_mnist.values()\n",
    "]\n",
    "\n",
    "for i, client_dataset in enumerate(client_datasets):\n",
    "    client_loader = torch.utils.data.DataLoader(client_dataset, batch_size=len(client_dataset))\n",
    "    client_samples, client_labels = next(iter(client_loader))\n",
    "    label_counts = Counter(client_labels.tolist())\n",
    "    print(f\"Client {i} label distribution: {dict(label_counts)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset: Non-IID / Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated 20 clients, each with 2280 training samples, and 12000 validation samples\n",
      "Client 0 label distribution: {5: 205}\n",
      "Client 1 label distribution: {6: 224}\n",
      "Client 2 label distribution: {1: 259}\n",
      "Client 3 label distribution: {5: 195}\n",
      "Client 4 label distribution: {4: 220}\n",
      "Client 5 label distribution: {7: 236}\n",
      "Client 6 label distribution: {6: 213}\n",
      "Client 7 label distribution: {7: 224}\n",
      "Client 8 label distribution: {6: 202}\n",
      "Client 9 label distribution: {6: 192}\n",
      "Client 10 label distribution: {9: 228}\n",
      "Client 11 label distribution: {8: 220}\n",
      "Client 12 label distribution: {6: 182}\n",
      "Client 13 label distribution: {8: 209}\n",
      "Client 14 label distribution: {4: 209}\n",
      "Client 15 label distribution: {3: 233}\n",
      "Client 16 label distribution: {3: 221}\n",
      "Client 17 label distribution: {3: 210}\n",
      "Client 18 label distribution: {4: 199}\n",
      "Client 19 label distribution: {3: 200}\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from collections import Counter\n",
    "\n",
    "# Number of clients and non-IID split\n",
    "num_clients = hp[\"num_clients\"]\n",
    "batch_size = hp[\"batch_size\"]\n",
    "classes_per_client = hp[\"classes_per_client\"]\n",
    "num_classes = hp[\"num_classes\"]\n",
    "\n",
    "train_data_size = len(mnist_train) // num_clients\n",
    "val_data_size = len(mnist_val) // num_clients\n",
    "\n",
    "if hp[\"split\"] == \"NONIID\":\n",
    "    \n",
    "    # Create indices for each class\n",
    "    train_class_indices = {i: np.where(np.array(mnist_train.targets) == i)[0] for i in range(num_classes)}\n",
    "\n",
    "    train_indices = []\n",
    "    for client_id in range(num_clients):\n",
    "        chosen_classes = np.random.choice(num_classes, classes_per_client, replace=False)\n",
    "        train_client_idx = []\n",
    "        val_client_idx = []\n",
    "        for cls in chosen_classes:\n",
    "            train_cls_size = max(1, len(train_class_indices[cls]) // (num_clients // classes_per_client))\n",
    "            \n",
    "            # Adjust for insufficient samples\n",
    "            train_cls_size = min(train_cls_size, len(train_class_indices[cls]))\n",
    "            \n",
    "            train_cls_idx = np.random.choice(train_class_indices[cls], train_cls_size, replace=False)\n",
    "            train_client_idx.extend(train_cls_idx)\n",
    "                        \n",
    "            # Remove assigned indices to avoid duplication\n",
    "            train_class_indices[cls] = np.setdiff1d(train_class_indices[cls], train_cls_idx)\n",
    "\n",
    "        train_indices.append(train_client_idx)\n",
    "\n",
    "    # Create datasets and DataLoaders for each client\n",
    "    train_dataset = [Subset(mnist_train, indices) for indices in train_indices]\n",
    "    train_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Split the training data into smaller datasets for each client\n",
    "    train_dataset = random_split(mnist_train, [train_data_size] * num_clients)\n",
    "    train_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]    \n",
    "\n",
    "# Test DataLoader for evaluation\n",
    "val_loader = DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Simulated {num_clients} clients, each with {train_data_size} training samples, and {len(mnist_val)} validation samples\")\n",
    "# Debugging: Output distribution of classes for all clients\n",
    "for i, client_dataset in enumerate(train_dataset):\n",
    "    client_loader = torch.utils.data.DataLoader(client_dataset, batch_size=len(client_dataset))\n",
    "    client_samples, client_labels = next(iter(client_loader))\n",
    "    label_counts = Counter(client_labels.tolist())\n",
    "    print(f\"Client {i} label distribution: {dict(label_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionally distribute the shared data among clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05    # Each client gets partition of the shared data\n",
    "\n",
    "shared_loader = torch.utils.data.DataLoader(mnist_train_shared, batch_size=len(mnist_train_shared))\n",
    "shared_samples, shared_labels = next(iter(shared_loader))\n",
    "num_shared_samples = int(alpha * len(shared_samples))\n",
    "\n",
    "merged_train_datasets = []\n",
    "for client_dataset in train_dataset:\n",
    "    client_loader = torch.utils.data.DataLoader(client_dataset, batch_size=len(client_dataset))\n",
    "    client_samples, client_labels = next(iter(client_loader))\n",
    "    \n",
    "    # Combine shared and local data\n",
    "    combined_samples = torch.cat([client_samples, shared_samples[:num_shared_samples]])\n",
    "    combined_labels = torch.cat([client_labels, shared_labels[:num_shared_samples]])\n",
    "    merged_train_dataset = torch.utils.data.TensorDataset(combined_samples, combined_labels)\n",
    "    merged_train_datasets.append(merged_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 label distribution: {3: 943, 1: 1050, 8: 12, 9: 12, 7: 10, 5: 8, 2: 16, 6: 10, 0: 7, 4: 12}\n",
      "Client 1 label distribution: {0: 909, 9: 928, 1: 19, 8: 12, 3: 14, 7: 10, 5: 8, 2: 16, 6: 10, 4: 12}\n",
      "Client 2 label distribution: {1: 843, 9: 745, 8: 12, 3: 14, 7: 10, 5: 8, 2: 16, 6: 10, 0: 7, 4: 12}\n",
      "Client 3 label distribution: {1: 679, 8: 893, 3: 14, 9: 12, 7: 10, 5: 8, 2: 16, 6: 10, 0: 7, 4: 12}\n",
      "Client 4 label distribution: {2: 922, 0: 729, 1: 19, 8: 12, 3: 14, 9: 12, 7: 10, 5: 8, 6: 10, 4: 12}\n",
      "Client 5 label distribution: {3: 757, 8: 717, 1: 19, 9: 12, 7: 10, 5: 8, 2: 16, 6: 10, 0: 7, 4: 12}\n",
      "Client 6 label distribution: {5: 830, 4: 897, 1: 19, 8: 12, 3: 14, 9: 12, 7: 10, 2: 16, 6: 10, 0: 7}\n",
      "Client 7 label distribution: {3: 609, 5: 666, 1: 19, 8: 12, 9: 12, 7: 10, 2: 16, 6: 10, 0: 7, 4: 12}\n",
      "Client 8 label distribution: {5: 534, 6: 909, 1: 19, 8: 12, 3: 14, 9: 12, 7: 10, 2: 16, 0: 7, 4: 12}\n",
      "Client 9 label distribution: {2: 741, 9: 599, 1: 19, 8: 12, 3: 14, 7: 10, 5: 8, 6: 10, 0: 7, 4: 12}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Display the label distribution for each client\n",
    "for i, client_dataset in enumerate(merged_train_datasets):\n",
    "    client_loader = torch.utils.data.DataLoader(client_dataset, batch_size=len(client_dataset))\n",
    "    client_samples, client_labels = next(iter(client_loader))\n",
    "    label_counts = Counter(client_labels.tolist())\n",
    "    print(f\"Client {i} label distribution: {dict(label_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in merged_train_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and initialize the Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size averaged_state: 407080 bytes (0.39 MB)\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialization: Instantiate the global model (server)\n",
    "global_model = QuantStubModel(q=hp['quantize'])\n",
    "if(hp[\"quantize\"]):\n",
    "    global_model.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "    torch.quantization.prepare_qat(global_model, inplace=True)\n",
    "\n",
    "model_size_bytes, model_size_mb = calculate_model_size(global_model)\n",
    "print(f\"Model size averaged_state: {model_size_bytes} bytes ({model_size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a training loop that is run on a client for a number of epochs\n",
    "def train_model(model, train_loader, hp, round_num, epochs=1, q=False, lambda_kure=0.0):\n",
    "\n",
    "    lr = hp[\"learning_rate\"]\n",
    "    delta = hp[\"delta\"]\n",
    "    bit_widths = hp[\"bit_widths\"]\n",
    "    \n",
    "    # 3. Distribution: Create a copy of the global model\n",
    "    local_model = QuantStubModel(q=q)\n",
    "    if(hp[\"quantize\"]):\n",
    "        local_model.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "        torch.quantization.prepare_qat(local_model, inplace=True)\n",
    "    local_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(local_model.parameters(), lr=hp[\"learning_rate\"])\n",
    "    \n",
    "    # Training loop\n",
    "    local_model.train()\n",
    "    total_loss = 0  # Initialize total loss\n",
    "    num_batches = 0  # Initialize batch counter\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if hp[\"averaging_setting\"] == 'mqat':\n",
    "                # Apply Pseudo-Quantization Noise (APQN)\n",
    "                if delta is not None:\n",
    "                    for param in model.parameters():\n",
    "                        param.data = add_pseudo_quantization_noise(param, delta)\n",
    "\n",
    "                # Apply Multi-Bit Quantization (MQAT)\n",
    "                if bit_widths is not None:\n",
    "                    bit_width = random.choice(bit_widths)\n",
    "                    for param in model.parameters():\n",
    "                        param.data = quantize_multi_bit(param, bit_width)\n",
    "\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Kurtosis Regularization\n",
    "            if hp[\"averaging_setting\"] == 'kure' and lambda_kure > 0:\n",
    "                for param in local_model.parameters():\n",
    "                    loss += lambda_kure * kurtosis_regularization(param)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    return local_model.state_dict(), avg_loss  # Return updated model parameters and average loss\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_loader, round_num):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #F.cross_entropy(outputs, labels, reduction='sum')  # Compute loss for the batch\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Resource Info (to simulate resource heterogeneity)\n",
    "client_resources = [{\"comp_capacity\": random.randint(10, 100), \"data_size\": random.randint(1, 10)} for _ in range(num_clients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Federated Learning Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Round 1\n",
      "-> Resource requested from 7 clients, 7 clients fulfilled the criteria\n",
      "Training loss after round 1: 0.7639188800205934\n",
      "Validation loss after round 1: 2.1827711912790932\n",
      "Global model updated for round 1\n",
      "Round 2\n",
      "-> Resource requested from 8 clients, 8 clients fulfilled the criteria\n",
      "Training loss after round 2: 0.659407128295625\n",
      "Validation loss after round 2: 1.9779454612731933\n",
      "Global model updated for round 2\n",
      "Round 3\n",
      "-> Resource requested from 7 clients, 7 clients fulfilled the criteria\n",
      "Training loss after round 3: 0.5069252590860052\n",
      "Validation loss after round 3: 1.5539732087453206\n",
      "Global model updated for round 3\n",
      "Round 4\n",
      "-> Resource requested from 9 clients, 9 clients fulfilled the criteria\n",
      "Training loss after round 4: 0.3705452310387804\n",
      "Validation loss after round 4: 0.8613528652985891\n",
      "Global model updated for round 4\n",
      "Round 5\n",
      "-> Resource requested from 1 clients, 1 clients fulfilled the criteria\n",
      "Training loss after round 5: 0.16300005562698994\n",
      "Validation loss after round 5: 1.1814951652685801\n",
      "Global model updated for round 5\n",
      "Round 6\n",
      "-> Resource requested from 6 clients, 6 clients fulfilled the criteria\n",
      "Training loss after round 6: 0.19836263880365276\n",
      "Validation loss after round 6: 0.697117374420166\n",
      "Global model updated for round 6\n",
      "Round 7\n",
      "-> Resource requested from 1 clients, 1 clients fulfilled the criteria\n",
      "Training loss after round 7: 0.06512128410872836\n",
      "Validation loss after round 7: 1.1325550855000814\n",
      "Global model updated for round 7\n",
      "Round 8\n",
      "-> Resource requested from 1 clients, 1 clients fulfilled the criteria\n",
      "Training loss after round 8: 0.035241981441728194\n",
      "Validation loss after round 8: 1.023363507270813\n",
      "Global model updated for round 8\n",
      "Round 9\n",
      "-> Resource requested from 1 clients, 1 clients fulfilled the criteria\n",
      "Training loss after round 9: 0.021391729205751265\n",
      "Validation loss after round 9: 1.517715506394704\n",
      "Global model updated for round 9\n",
      "Round 10\n",
      "-> Resource requested from 10 clients, 10 clients fulfilled the criteria\n",
      "Training loss after round 10: 0.3932525678380601\n",
      "Validation loss after round 10: 0.8844509160518647\n",
      "Global model updated for round 10\n",
      "Total time taken: 51.10 seconds\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Generate a unique log directory based on the current time\n",
    "run_number = hp[\"run_number\"]\n",
    "log_dir = f\"./logs/{hp['run_id']}/run_{run_number}-{datetime.now().strftime('%m-%d-%H-%M')}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Log hyperparameters as text\n",
    "hyperparams_text = \"\\n\".join([f\"{key}: {value}\" for key, value in hp.items()])\n",
    "\n",
    "# Start time measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Federated Learning with FedCS Client Selection\n",
    "num_rounds = hp[\"num_rounds\"]\n",
    "epochs = hp[\"epochs\"]\n",
    "round_deadline = 3  # Example round deadline (in arbitrary time units)\n",
    "\n",
    "assert num_clients == len(train_loader)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "# Conduct federated learning rounds\n",
    "for round_num in range(num_rounds):\n",
    "    print(f\"Round {round_num + 1}\")\n",
    "    \n",
    "    # 2. Resource Request\n",
    "    k = random.randint(1, num_clients)\n",
    "    resource_requested_clients = select_indices(num_clients, k)\n",
    "\n",
    "    # 2. Client Selection: Collect client updates\n",
    "    # TODO: Client ressources\n",
    "    selected_train_clients = client_selection_with_constraints([client_resources[i] for i in resource_requested_clients], round_deadline)\n",
    "    filtered_train_loaders = [train_loader[i] for i in selected_train_clients]\n",
    "    #filtered_val_loaders = [val_loader[i] for i in selected_train_clients]\n",
    "    print(f\"-> Resource requested from {len(resource_requested_clients)} clients, {len(selected_train_clients)} clients fulfilled the criteria\")\n",
    "\n",
    "    client_states = []\n",
    "    round_train_loss = 0  # Initialize round loss\n",
    "    round_val_loss = 0  # Initialize round loss\n",
    "    num_batches = 0  # Initialize batch counter\n",
    "\n",
    "    #for client_train_loader, client_val_loader in zip(filtered_train_loaders, filtered_val_loaders):\n",
    "    for client_train_loader in filtered_train_loaders:\n",
    "\n",
    "        # 4. Distribution\n",
    "        client_state, client_loss = train_model(global_model, client_train_loader, hp, round_num, epochs=epochs, q=hp[\"quantize\"], lambda_kure=hp[\"lambda_kure\"])\n",
    "        round_train_loss += client_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        # 5. Update and Upload\n",
    "        client_states.append(client_state)\n",
    "\n",
    "    # Average loss for the round\n",
    "    avg_round_training_loss = round_train_loss / num_batches if num_batches > 0 else 0\n",
    "    writer.add_scalar(\"Metrics/Training loss\", avg_round_training_loss, round_num + 1)\n",
    "    print(f\"Training loss after round {round_num + 1}: {avg_round_training_loss}\")\n",
    "\n",
    "    # 6. Aggregation: Aggregate updates using Federated Averaging\n",
    "    new_global_state = fl.federated_averaging(global_model, client_states, setting=hp[\"averaging_setting\"])\n",
    "    global_model.load_state_dict(new_global_state)\n",
    "\n",
    "    #val_loss = validate_model(global_model, client_val_loader, round_num)\n",
    "    val_loss = validate_model(global_model, val_loader, round_num)\n",
    "\n",
    "    writer.add_scalar(\"Metrics/Validation loss\", val_loss, round_num + 1)\n",
    "    print(f\"Validation loss after round {round_num + 1}: {val_loss}\")\n",
    "    print(f\"Global model updated for round {round_num + 1}\")\n",
    "\n",
    "if hp[\"quantize\"]:\n",
    "    torch.quantization.convert(global_model, inplace=True)\n",
    "\n",
    "# End time measurement\n",
    "end_time = time.time()\n",
    "\n",
    "# Print total execution time\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model accuracy: 73.32%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print(f\"Global model accuracy: {test_accuracy:.2%}\")\n",
    "    return test_accuracy\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_accuracy = evaluate_model(global_model, test_loader)\n",
    "\n",
    "# End TensorBoard writer\n",
    "final_metrics = {}\n",
    "hp_cleaned = {k: v for k, v in hp.items() if isinstance(v, (int, float, str, bool, torch.Tensor))}\n",
    "writer.add_hparams(hp_cleaned, final_metrics)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model saved at ./saved_models/test/global_model_test_qunatized_mqat_32168_noniid.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_save_path = f\"./saved_models/{hp['run_id']}/global_model_{hp['run_id']}_{run_number}.pth\"\n",
    "torch.save(global_model.state_dict(), model_save_path)\n",
    "print(f\"Global model saved at {model_save_path}\")\n",
    "#model_size_bytes, model_size_mb = calculate_model_size(global_model)\n",
    "#print(f\"Model size: {model_size_bytes} bytes ({model_size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

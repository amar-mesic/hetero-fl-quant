{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Get the absolute path of the src directory\n",
    "src_path = os.path.abspath('../src')\n",
    "\n",
    "# Add src_path to sys.path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "import fl\n",
    "from fl import FedCSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 48000, Validation dataset size: 12000, Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load MNIST dataset\n",
    "mnist_full = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"../data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(mnist_full))  # 80% for training\n",
    "val_size = len(mnist_full) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(mnist_full, [train_size, val_size])\n",
    "\n",
    "# Extract indices from Subset objects\n",
    "train_indices = train_dataset.indices  # List of training indices\n",
    "val_indices = val_dataset.indices      # List of validation indices\n",
    "\n",
    "# Create training and validation MNIST datasets\n",
    "mnist_train = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=False)\n",
    "mnist_val = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=False)\n",
    "\n",
    "# Filter datasets by indices\n",
    "mnist_train.data = mnist_train.data[torch.tensor(train_indices)]\n",
    "mnist_train.targets = mnist_train.targets[torch.tensor(train_indices)]\n",
    "mnist_val.data = mnist_val.data[torch.tensor(val_indices)]\n",
    "mnist_val.targets = mnist_val.targets[torch.tensor(val_indices)]\n",
    "\n",
    "# Create DataLoaders for training and validation datasets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(mnist_train)}, Validation dataset size: {len(mnist_val)}, Test dataset size: {len(mnist_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hp = {\n",
    "    \"run_id\": \"delta\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_clients\": 10,\n",
    "    \"num_rounds\": 20,\n",
    "    \"num_classes\": 10,\n",
    "    \"classes_per_client\": 2,\n",
    "    \"epochs\": 3,    # number of epochs to train in each round\n",
    "    \"split\": \"RANDOM\"   # [\"RANDOM\", \"NONIID\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset: Non-IID / Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated 10 clients, each with 4800 training samples.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Number of clients and non-IID split\n",
    "num_clients = hp[\"num_clients\"]\n",
    "batch_size = hp[\"batch_size\"]\n",
    "train_data_size = len(mnist_train) // num_clients\n",
    "val_data_size = len(mnist_val) // num_clients\n",
    "\n",
    "\n",
    "if hp[\"split\"] == \"NONIID\":\n",
    "    classes_per_client = hp[\"classes_per_client\"]\n",
    "    num_classes = hp[\"num_classes\"]\n",
    "\n",
    "    # Create indices for each class\n",
    "    train_class_indices = {i: np.where(np.array(mnist_train.targets) == i)[0] for i in range(num_classes)}\n",
    "    val_class_indices = {i: np.where(np.array(mnist_val.targets) == i)[0] for i in range(num_classes)}\n",
    "\n",
    "    # Assign 2 classes per client\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    for client_id in range(num_clients):\n",
    "        chosen_classes = np.random.choice(num_classes, classes_per_client, replace=False)\n",
    "        train_client_idx = []\n",
    "        val_client_idx = []\n",
    "        for cls in chosen_classes:\n",
    "            train_cls_size = len(train_class_indices[cls]) // (num_clients // classes_per_client)\n",
    "            train_cls_idx = np.random.choice(train_class_indices[cls], train_cls_size, replace=False)\n",
    "            train_client_idx.extend(train_cls_idx)\n",
    "            val_cls_size = len(val_class_indices[cls]) // (num_clients // classes_per_client)\n",
    "            val_cls_idx = np.random.choice(val_class_indices[cls], val_cls_size, replace=False)\n",
    "            val_client_idx.extend(val_cls_idx)\n",
    "            # Remove assigned indices to avoid duplication\n",
    "            train_class_indices[cls] = np.setdiff1d(train_class_indices[cls], train_cls_idx)\n",
    "            val_class_indices[cls] = np.setdiff1d(val_class_indices[cls], val_cls_idx)\n",
    "\n",
    "        train_indices.append(train_client_idx)\n",
    "        val_indices.append(val_client_idx)\n",
    "\n",
    "    # Create datasets and DataLoaders for each client\n",
    "    train_dataset = [Subset(mnist_train, indices) for indices in train_indices]\n",
    "    train_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]\n",
    "    val_dataset = [Subset(mnist_val, indices) for indices in val_indices]\n",
    "    val_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in val_dataset]\n",
    "\n",
    "    # Example: Check the distribution of classes for a specific client\n",
    "    train_sample_classes = [mnist_train.targets[idx].item() for idx in train_indices[0]]\n",
    "    print(\"Client 0 has classes:\", set(train_sample_classes))\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Split the training data into smaller datasets for each client\n",
    "    train_dataset = random_split(mnist_train, [train_data_size] * num_clients)\n",
    "    train_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]\n",
    "\n",
    "    val_dataset = random_split(mnist_train, [train_data_size] * num_clients)\n",
    "    val_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]\n",
    "\n",
    "# Test DataLoader for evaluation\n",
    "print(f\"Simulated {num_clients} clients, each with {train_data_size} training samples.\")\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and initialize the Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nobis\\anaconda3\\envs\\feder\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FedCSModel(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (fc1): Linear(\n",
       "    in_features=784, out_features=128, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=10, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Initialization: Instantiate the global model (server)\n",
    "global_model = FedCSModel(q=True)\n",
    "global_model.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare_qat(global_model, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a training loop that is run on a client for a number of epochs\n",
    "def train_model(model, train_loader, hp, epochs=1):\n",
    "    \n",
    "    lr = hp[\"learning_rate\"]\n",
    "\n",
    "    # 3. Distribution: Create a copy of the global model\n",
    "    local_model = FedCSModel(q=True)\n",
    "    local_model.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "    torch.quantization.prepare_qat(local_model, inplace=True)\n",
    "    local_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(local_model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    local_model.train()\n",
    "    total_loss = 0  # Initialize total loss\n",
    "    num_batches = 0  # Initialize batch counter\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    #torch.quantization.convert(local_model, inplace=True)\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    return local_model.state_dict(), avg_loss  # Return updated model parameters and average loss\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #F.cross_entropy(outputs, labels, reduction='sum')  # Compute loss for the batch\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Resource Info (to simulate resource heterogeneity)\n",
    "client_resources = [{\"comp_capacity\": random.randint(10, 100), \"data_size\": random.randint(1, 10)} for _ in range(num_clients)]\n",
    "\n",
    "def client_selection_with_constraints(client_resources, deadline):\n",
    "    \"\"\"\n",
    "    Select clients based on their resource availability and time constraints.\n",
    "    \"\"\"\n",
    "    selected_clients = []\n",
    "    total_time = 0  # Track elapsed time\n",
    "    remaining_clients = list(range(len(client_resources)))  # Indices of available clients\n",
    "\n",
    "    while remaining_clients:\n",
    "        # Sort remaining clients by minimum time to complete training and upload\n",
    "        best_client = None\n",
    "        min_time = float('inf')\n",
    "\n",
    "        for client in remaining_clients:\n",
    "            resource = client_resources[client]\n",
    "            update_time = resource[\"data_size\"] / resource[\"comp_capacity\"]  # Simplified time calculation\n",
    "            if total_time + update_time < deadline and update_time < min_time:\n",
    "                best_client = client\n",
    "                min_time = update_time\n",
    "\n",
    "        if best_client is None:\n",
    "            break  # No more clients can be selected within the deadline\n",
    "\n",
    "        # Select the best client\n",
    "        selected_clients.append(best_client)\n",
    "        total_time += min_time\n",
    "        remaining_clients.remove(best_client)\n",
    "\n",
    "    return selected_clients\n",
    "\n",
    "\n",
    "def select_indices(n, k):\n",
    "    return random.sample(range(n), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Federated Learning Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Round 1\n",
      "-- Resource requested from 10 clients\n",
      "-- Filtered 10 clients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nobis\\anaconda3\\envs\\feder\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss after round 1: 0.7076105917609399\n",
      "Global model updated for round 1\n",
      "Validation loss after round 1: 2.3298020237286883\n",
      "Round 2\n",
      "-- Resource requested from 5 clients\n",
      "-- Filtered 5 clients\n",
      "Training loss after round 2: 0.3503669883691602\n",
      "Global model updated for round 2\n",
      "Validation loss after round 2: 0.3663866283198198\n",
      "Round 3\n",
      "-- Resource requested from 4 clients\n",
      "-- Filtered 4 clients\n",
      "Training loss after round 3: 0.24976531516036227\n",
      "Global model updated for round 3\n",
      "Validation loss after round 3: 0.22389184047468008\n",
      "Round 4\n",
      "-- Resource requested from 5 clients\n",
      "-- Filtered 5 clients\n",
      "Training loss after round 4: 0.205326588033802\n",
      "Global model updated for round 4\n",
      "Validation loss after round 4: 0.17989828748752673\n",
      "Round 5\n",
      "-- Resource requested from 4 clients\n",
      "-- Filtered 4 clients\n",
      "Training loss after round 5: 0.16651358960532686\n",
      "Global model updated for round 5\n",
      "Validation loss after round 5: 0.1478503160442536\n",
      "Round 6\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 6: 0.13798399036698458\n",
      "Global model updated for round 6\n",
      "Validation loss after round 6: 0.11867585433026155\n",
      "Round 7\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 7: 0.14137892980808542\n",
      "Global model updated for round 7\n",
      "Validation loss after round 7: 0.13769465198917757\n",
      "Round 8\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 8: 0.09446142995600693\n",
      "Global model updated for round 8\n",
      "Validation loss after round 8: 0.07564708082160601\n",
      "Round 9\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 9: 0.07596850163458536\n",
      "Global model updated for round 9\n",
      "Validation loss after round 9: 0.05814606182121982\n",
      "Round 10\n",
      "-- Resource requested from 5 clients\n",
      "-- Filtered 5 clients\n",
      "Training loss after round 10: 0.10557630450609659\n",
      "Global model updated for round 10\n",
      "Validation loss after round 10: 0.12782418753020464\n",
      "Round 11\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 11: 0.03803633530520731\n",
      "Global model updated for round 11\n",
      "Validation loss after round 11: 0.036444466731821494\n",
      "Round 12\n",
      "-- Resource requested from 4 clients\n",
      "-- Filtered 4 clients\n",
      "Training loss after round 12: 0.08269830471716381\n",
      "Global model updated for round 12\n",
      "Validation loss after round 12: 0.1105697382237607\n",
      "Round 13\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 13: 0.056736503006131554\n",
      "Global model updated for round 13\n",
      "Validation loss after round 13: 0.050181116174078645\n",
      "Round 14\n",
      "-- Resource requested from 8 clients\n",
      "-- Filtered 8 clients\n",
      "Training loss after round 14: 0.09448622796320708\n",
      "Global model updated for round 14\n",
      "Validation loss after round 14: 0.08396822104933867\n",
      "Round 15\n",
      "-- Resource requested from 10 clients\n",
      "-- Filtered 10 clients\n",
      "Training loss after round 15: 0.09325795176193222\n",
      "Global model updated for round 15\n",
      "Validation loss after round 15: 0.08098246790536602\n",
      "Round 16\n",
      "-- Resource requested from 9 clients\n",
      "-- Filtered 9 clients\n",
      "Training loss after round 16: 0.07876928652365071\n",
      "Global model updated for round 16\n",
      "Validation loss after round 16: 0.06871418872365245\n",
      "Round 17\n",
      "-- Resource requested from 8 clients\n",
      "-- Filtered 8 clients\n",
      "Training loss after round 17: 0.06926562809163847\n",
      "Global model updated for round 17\n",
      "Validation loss after round 17: 0.05960666855462478\n",
      "Round 18\n",
      "-- Resource requested from 10 clients\n",
      "-- Filtered 10 clients\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This took me 2 mins to run\n",
    "Simulate Federated Learning\n",
    "A learning round consists of all clients training their local models and then aggregating the updates\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Generate a unique log directory based on the current time\n",
    "run_number = datetime.now().strftime('%m-%d-%H-%M')  # Month-Day_Hour-Minute\n",
    "log_dir = f\"./logs/{hp['run_id']}/run_{run_number}\"  # Use a timestamp to distinguish runs\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Log hyperparameters as text\n",
    "hyperparams_text = \"\\n\".join([f\"{key}: {value}\" for key, value in hp.items()])\n",
    "\n",
    "# Start time measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Federated Learning with FedCS Client Selection\n",
    "num_rounds = hp[\"num_rounds\"]\n",
    "epochs = hp[\"epochs\"]\n",
    "round_deadline = 3  # Example round deadline (in arbitrary time units)\n",
    "\n",
    "assert num_clients == len(train_loader)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "# Conduct federated learning rounds\n",
    "for round_num in range(num_rounds):\n",
    "    print(f\"Round {round_num + 1}\")\n",
    "    \n",
    "    # 2. Resource Request\n",
    "    k = random.randint(1, num_clients)\n",
    "    resource_requested_clients = select_indices(num_clients, k)\n",
    "    print(f\"-- Resource requested from {len(resource_requested_clients)} clients\")\n",
    "\n",
    "    # 2. Client Selection: Collect client updates\n",
    "    # TODO: Client ressources\n",
    "    selected_train_clients = client_selection_with_constraints([client_resources[i] for i in resource_requested_clients], round_deadline)\n",
    "    print(f\"-- Filtered {len(selected_train_clients)} clients\")\n",
    "    filtered_train_loaders = [train_loader[i] for i in selected_train_clients]\n",
    "    filtered_val_loaders = [val_loader[i] for i in selected_train_clients]\n",
    "\n",
    "    client_states = []\n",
    "    round_train_loss = 0  # Initialize round loss\n",
    "    round_val_loss = 0  # Initialize round loss\n",
    "    num_batches = 0  # Initialize batch counter\n",
    "\n",
    "    for client_train_loader, client_val_loader in zip(filtered_train_loaders, filtered_val_loaders):\n",
    "        \n",
    "        # 4. Distribution\n",
    "        client_state, client_loss = train_model(global_model, client_train_loader, hp, epochs=epochs)\n",
    "\n",
    "        # Log client loss\n",
    "        round_train_loss += client_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss = validate_model(global_model, client_val_loader)\n",
    "        round_val_loss += val_loss\n",
    "\n",
    "        # 5. Update and Upload\n",
    "        client_states.append(client_state)\n",
    "\n",
    "    # Average loss for the round\n",
    "    avg_round_training_loss = round_train_loss / num_batches if num_batches > 0 else 0\n",
    "    writer.add_scalar(\"Metrics/Training loss\", avg_round_training_loss, round_num + 1)\n",
    "    print(f\"Training loss after round {round_num + 1}: {avg_round_training_loss}\")\n",
    "\n",
    "    # 6. Aggregation: Aggregate updates using Federated Averaging\n",
    "    new_global_state = fl.federated_averaging(global_model, client_states)  \n",
    "    global_model.load_state_dict(new_global_state)\n",
    "    print(f\"Global model updated for round {round_num + 1}\")\n",
    "\n",
    "    avg_round_val_loss = round_val_loss / num_batches if num_batches > 0 else 0\n",
    "    writer.add_scalar(\"Metrics/Validation loss\", avg_round_val_loss, round_num + 1)\n",
    "    print(f\"Validation loss after round {round_num + 1}: {avg_round_val_loss}\")\n",
    "\n",
    "torch.quantization.convert(global_model, inplace=True)\n",
    "\n",
    "# End time measurement\n",
    "end_time = time.time()\n",
    "\n",
    "# Print total execution time\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model:\n",
      "FedCSModel(\n",
      "  (quant): Quantize(scale=tensor([0.0157]), zero_point=tensor([64]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (fc1): QuantizedLinear(in_features=784, out_features=128, scale=0.1566275954246521, zero_point=70, qscheme=torch.per_channel_affine)\n",
      "  (relu): ReLU()\n",
      "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.2665318250656128, zero_point=58, qscheme=torch.per_channel_affine)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the quantized model\n",
    "print(\"Quantized model:\")\n",
    "print(global_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model accuracy: 96.73%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print(f\"Global model accuracy: {test_accuracy:.2%}\")\n",
    "    return test_accuracy\n",
    "    \n",
    "# Evaluate the model on the test dataset\n",
    "test_accuracy = evaluate_model(global_model, test_loader)\n",
    "\n",
    "# End TensorBoard writer\n",
    "final_metrics = {}\n",
    "writer.add_hparams(hp, final_metrics)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model saved at ./saved_models/global_model_delta_11-26-13-49.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_save_path = f\"./saved_models/global_model_{hp['run_id']}_{run_number}.pth\"\n",
    "torch.save(global_model.state_dict(), model_save_path)\n",
    "print(f\"Global model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Get the absolute path of the src directory\n",
    "src_path = os.path.abspath('../src')\n",
    "\n",
    "# Add src_path to sys.path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "import fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Tensorboard notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 48000, Validation dataset size: 12000, Test dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load MNIST dataset\n",
    "mnist_full = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(root=\"../data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.8 * len(mnist_full))  # 80% for training\n",
    "val_size = len(mnist_full) - train_size  # 20% for validation\n",
    "train_dataset, val_dataset = random_split(mnist_full, [train_size, val_size])\n",
    "\n",
    "# Extract indices from Subset objects\n",
    "train_indices = train_dataset.indices  # List of training indices\n",
    "val_indices = val_dataset.indices      # List of validation indices\n",
    "\n",
    "# Create training and validation MNIST datasets\n",
    "mnist_train = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=False)\n",
    "mnist_val = datasets.MNIST(root=\"../data\", train=True, transform=transform, download=False)\n",
    "\n",
    "# Filter datasets by indices\n",
    "mnist_train.data = mnist_train.data[torch.tensor(train_indices)]\n",
    "mnist_train.targets = mnist_train.targets[torch.tensor(train_indices)]\n",
    "mnist_val.data = mnist_val.data[torch.tensor(val_indices)]\n",
    "mnist_val.targets = mnist_val.targets[torch.tensor(val_indices)]\n",
    "\n",
    "# Create DataLoaders for training and validation datasets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(mnist_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(mnist_train)}, Validation dataset size: {len(mnist_val)}, Test dataset size: {len(mnist_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hp = {\n",
    "    \"run_id\": \"gamma\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_clients\": 20,\n",
    "    \"num_rounds\": 100,\n",
    "    \"num_classes\": 10,\n",
    "    \"classes_per_client\": 6,\n",
    "    \"epochs\": 3,    # number of epochs to train in each round\n",
    "    \"split\": \"RANDOM\"   # [\"RANDOM\", \"NONIID\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset: non-IID / Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated 20 clients, each with 2400 training samples.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Number of clients and non-IID split\n",
    "num_clients = hp[\"num_clients\"]\n",
    "batch_size = hp[\"batch_size\"]\n",
    "train_data_size = len(mnist_train) // num_clients\n",
    "val_data_size = len(mnist_val) // num_clients\n",
    "\n",
    "\n",
    "if hp[\"split\"] == \"NONIID\":\n",
    "    classes_per_client = hp[\"classes_per_client\"]\n",
    "    num_classes = hp[\"num_classes\"]\n",
    "\n",
    "    # Create indices for each class\n",
    "    train_class_indices = {i: np.where(np.array(mnist_train.targets) == i)[0] for i in range(num_classes)}\n",
    "    val_class_indices = {i: np.where(np.array(mnist_val.targets) == i)[0] for i in range(num_classes)}\n",
    "\n",
    "    # Assign 2 classes per client\n",
    "    train_indices = []\n",
    "    val_indices = []\n",
    "    for client_id in range(num_clients):\n",
    "        chosen_classes = np.random.choice(num_classes, classes_per_client, replace=False)\n",
    "        train_client_idx = []\n",
    "        val_client_idx = []\n",
    "        for cls in chosen_classes:\n",
    "            train_cls_size = len(train_class_indices[cls]) // (num_clients // classes_per_client)\n",
    "            train_cls_idx = np.random.choice(train_class_indices[cls], train_cls_size, replace=False)\n",
    "            train_client_idx.extend(train_cls_idx)\n",
    "            val_cls_size = len(val_class_indices[cls]) // (num_clients // classes_per_client)\n",
    "            val_cls_idx = np.random.choice(val_class_indices[cls], val_cls_size, replace=False)\n",
    "            val_client_idx.extend(val_cls_idx)\n",
    "            # Remove assigned indices to avoid duplication\n",
    "            train_class_indices[cls] = np.setdiff1d(train_class_indices[cls], train_cls_idx)\n",
    "            val_class_indices[cls] = np.setdiff1d(val_class_indices[cls], val_cls_idx)\n",
    "\n",
    "        train_indices.append(train_client_idx)\n",
    "        val_indices.append(val_client_idx)\n",
    "\n",
    "    # Create datasets and DataLoaders for each client\n",
    "    train_dataset = [Subset(mnist_train, indices) for indices in train_indices]\n",
    "    train_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]\n",
    "    val_dataset = [Subset(mnist_val, indices) for indices in val_indices]\n",
    "    val_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in val_dataset]\n",
    "\n",
    "    # Example: Check the distribution of classes for a specific client\n",
    "    train_sample_classes = [mnist_train.targets[idx].item() for idx in train_indices[0]]\n",
    "    print(\"Client 0 has classes:\", set(train_sample_classes))\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Split the training data into smaller datasets for each client\n",
    "    train_dataset = random_split(mnist_train, [train_data_size] * num_clients)\n",
    "    train_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]\n",
    "\n",
    "    val_dataset = random_split(mnist_train, [train_data_size] * num_clients)\n",
    "    val_loader = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_dataset]\n",
    "\n",
    "# Test DataLoader for evaluation\n",
    "print(f\"Simulated {num_clients} clients, each with {train_data_size} training samples.\")\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in val_loader[0]:  # Example for the first client\n",
    "    print(inputs.shape, labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and initialize the Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialization: Instantiate the global model (server)\n",
    "model = fl.create_model()\n",
    "global_model = model\n",
    "print(global_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a training loop that is run on a client for a number of epochs\n",
    "def train_model(model, train_loader, hp, epochs=1):\n",
    "    \n",
    "    lr = hp[\"learning_rate\"]\n",
    "\n",
    "    # 3. Distribution: Create a copy of the global model\n",
    "    local_model = fl.create_model()\n",
    "    local_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(local_model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    local_model.train()\n",
    "    total_loss = 0  # Initialize total loss\n",
    "    num_batches = 0  # Initialize batch counter\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    return local_model.state_dict(), avg_loss  # Return updated model parameters and average loss\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #F.cross_entropy(outputs, labels, reduction='sum')  # Compute loss for the batch\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Resource Info (to simulate resource heterogeneity)\n",
    "client_resources = [{\"comp_capacity\": random.randint(10, 100), \"data_size\": random.randint(1, 10)} for _ in range(num_clients)]\n",
    "\n",
    "def client_selection_with_constraints(client_resources, deadline):\n",
    "    \"\"\"\n",
    "    Select clients based on their resource availability and time constraints.\n",
    "    \"\"\"\n",
    "    selected_clients = []\n",
    "    total_time = 0  # Track elapsed time\n",
    "    remaining_clients = list(range(len(client_resources)))  # Indices of available clients\n",
    "\n",
    "    while remaining_clients:\n",
    "        # Sort remaining clients by minimum time to complete training and upload\n",
    "        best_client = None\n",
    "        min_time = float('inf')\n",
    "\n",
    "        for client in remaining_clients:\n",
    "            resource = client_resources[client]\n",
    "            update_time = resource[\"data_size\"] / resource[\"comp_capacity\"]  # Simplified time calculation\n",
    "            if total_time + update_time < deadline and update_time < min_time:\n",
    "                best_client = client\n",
    "                min_time = update_time\n",
    "\n",
    "        if best_client is None:\n",
    "            break  # No more clients can be selected within the deadline\n",
    "\n",
    "        # Select the best client\n",
    "        selected_clients.append(best_client)\n",
    "        total_time += min_time\n",
    "        remaining_clients.remove(best_client)\n",
    "\n",
    "    return selected_clients\n",
    "\n",
    "\n",
    "def select_indices(n, k):\n",
    "    return random.sample(range(n), k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Federated Learning Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "Round 1\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 1: 0.8680421385279408\n",
      "Global model updated for round 1\n",
      "Validation loss after round 1: 2.3230015426211885\n",
      "Round 2\n",
      "-- Resource requested from 17 clients\n",
      "-- Filtered 17 clients\n",
      "Training loss after round 2: 0.39353951110948926\n",
      "Global model updated for round 2\n",
      "Validation loss after round 2: 0.41924699326356246\n",
      "Round 3\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 3: 0.2972545917320346\n",
      "Global model updated for round 3\n",
      "Validation loss after round 3: 0.27297349411107247\n",
      "Round 4\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 4: 0.24398201250367693\n",
      "Global model updated for round 4\n",
      "Validation loss after round 4: 0.22042343487342198\n",
      "Round 5\n",
      "-- Resource requested from 6 clients\n",
      "-- Filtered 6 clients\n",
      "Training loss after round 5: 0.20825358107961991\n",
      "Global model updated for round 5\n",
      "Validation loss after round 5: 0.2733915178270803\n",
      "Round 6\n",
      "-- Resource requested from 13 clients\n",
      "-- Filtered 13 clients\n",
      "Training loss after round 6: 0.18602923847432448\n",
      "Global model updated for round 6\n",
      "Validation loss after round 6: 0.17682705628327453\n",
      "Round 7\n",
      "-- Resource requested from 20 clients\n",
      "-- Filtered 20 clients\n",
      "Training loss after round 7: 0.17404572467754284\n",
      "Global model updated for round 7\n",
      "Validation loss after round 7: 0.1584347967955594\n",
      "Round 8\n",
      "-- Resource requested from 8 clients\n",
      "-- Filtered 8 clients\n",
      "Training loss after round 8: 0.14360441523785186\n",
      "Global model updated for round 8\n",
      "Validation loss after round 8: 0.1289033345753948\n",
      "Round 9\n",
      "-- Resource requested from 5 clients\n",
      "-- Filtered 5 clients\n",
      "Training loss after round 9: 0.12706696370575163\n",
      "Global model updated for round 9\n",
      "Validation loss after round 9: 0.11324546816945077\n",
      "Round 10\n",
      "-- Resource requested from 20 clients\n",
      "-- Filtered 20 clients\n",
      "Training loss after round 10: 0.13817524413081508\n",
      "Global model updated for round 10\n",
      "Validation loss after round 10: 0.12997357410068314\n",
      "Round 11\n",
      "-- Resource requested from 14 clients\n",
      "-- Filtered 14 clients\n",
      "Training loss after round 11: 0.11953922788346454\n",
      "Global model updated for round 11\n",
      "Validation loss after round 11: 0.10919577371462116\n",
      "Round 12\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 12: 0.09912841206830408\n",
      "Global model updated for round 12\n",
      "Validation loss after round 12: 0.08364450925547216\n",
      "Round 13\n",
      "-- Resource requested from 8 clients\n",
      "-- Filtered 8 clients\n",
      "Training loss after round 13: 0.09490257243457664\n",
      "Global model updated for round 13\n",
      "Validation loss after round 13: 0.09184261853341012\n",
      "Round 14\n",
      "-- Resource requested from 12 clients\n",
      "-- Filtered 12 clients\n",
      "Training loss after round 14: 0.09559747007304664\n",
      "Global model updated for round 14\n",
      "Validation loss after round 14: 0.08943538957016749\n",
      "Round 15\n",
      "-- Resource requested from 17 clients\n",
      "-- Filtered 17 clients\n",
      "Training loss after round 15: 0.09925109677841534\n",
      "Global model updated for round 15\n",
      "Validation loss after round 15: 0.09327952723457096\n",
      "Round 16\n",
      "-- Resource requested from 18 clients\n",
      "-- Filtered 18 clients\n",
      "Training loss after round 16: 0.09456448236693066\n",
      "Global model updated for round 16\n",
      "Validation loss after round 16: 0.08516933949452962\n",
      "Round 17\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 17: 0.053729285304951996\n",
      "Global model updated for round 17\n",
      "Validation loss after round 17: 0.049627777667095266\n",
      "Round 18\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 18: 0.059250343026144923\n",
      "Global model updated for round 18\n",
      "Validation loss after round 18: 0.06512095334111816\n",
      "Round 19\n",
      "-- Resource requested from 18 clients\n",
      "-- Filtered 18 clients\n",
      "Training loss after round 19: 0.08861267324560033\n",
      "Global model updated for round 19\n",
      "Validation loss after round 19: 0.08757309897962301\n",
      "Round 20\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 20: 0.020417216044022805\n",
      "Global model updated for round 20\n",
      "Validation loss after round 20: 0.02330656404296557\n",
      "Round 21\n",
      "-- Resource requested from 9 clients\n",
      "-- Filtered 9 clients\n",
      "Training loss after round 21: 0.07228244915420627\n",
      "Global model updated for round 21\n",
      "Validation loss after round 21: 0.08032773461287704\n",
      "Round 22\n",
      "-- Resource requested from 9 clients\n",
      "-- Filtered 9 clients\n",
      "Training loss after round 22: 0.06466305808178552\n",
      "Global model updated for round 22\n",
      "Validation loss after round 22: 0.057849174597973216\n",
      "Round 23\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 23: 0.013968567722477019\n",
      "Global model updated for round 23\n",
      "Validation loss after round 23: 0.017601172880580027\n",
      "Round 24\n",
      "-- Resource requested from 11 clients\n",
      "-- Filtered 11 clients\n",
      "Training loss after round 24: 0.06699678973938018\n",
      "Global model updated for round 24\n",
      "Validation loss after round 24: 0.06724392631712059\n",
      "Round 25\n",
      "-- Resource requested from 11 clients\n",
      "-- Filtered 11 clients\n",
      "Training loss after round 25: 0.05908710592273492\n",
      "Global model updated for round 25\n",
      "Validation loss after round 25: 0.05416172579844539\n",
      "Round 26\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 26: 0.008531315727676783\n",
      "Global model updated for round 26\n",
      "Validation loss after round 26: 0.014794709701867153\n",
      "Round 27\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 27: 0.03294866906918792\n",
      "Global model updated for round 27\n",
      "Validation loss after round 27: 0.035055438928150884\n",
      "Round 28\n",
      "-- Resource requested from 8 clients\n",
      "-- Filtered 8 clients\n",
      "Training loss after round 28: 0.05239712906351391\n",
      "Global model updated for round 28\n",
      "Validation loss after round 28: 0.0487972836062545\n",
      "Round 29\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 29: 0.04253689305583341\n",
      "Global model updated for round 29\n",
      "Validation loss after round 29: 0.04056327674615507\n",
      "Round 30\n",
      "-- Resource requested from 18 clients\n",
      "-- Filtered 18 clients\n",
      "Training loss after round 30: 0.07126033212416116\n",
      "Global model updated for round 30\n",
      "Validation loss after round 30: 0.06499485536420252\n",
      "Round 31\n",
      "-- Resource requested from 10 clients\n",
      "-- Filtered 10 clients\n",
      "Training loss after round 31: 0.04414181033001902\n",
      "Global model updated for round 31\n",
      "Validation loss after round 31: 0.04298365131482327\n",
      "Round 32\n",
      "-- Resource requested from 16 clients\n",
      "-- Filtered 16 clients\n",
      "Training loss after round 32: 0.05995880481017391\n",
      "Global model updated for round 32\n",
      "Validation loss after round 32: 0.0542853052020655\n",
      "Round 33\n",
      "-- Resource requested from 6 clients\n",
      "-- Filtered 6 clients\n",
      "Training loss after round 33: 0.030688296058818832\n",
      "Global model updated for round 33\n",
      "Validation loss after round 33: 0.030474814300735797\n",
      "Round 34\n",
      "-- Resource requested from 5 clients\n",
      "-- Filtered 5 clients\n",
      "Training loss after round 34: 0.02599031690120076\n",
      "Global model updated for round 34\n",
      "Validation loss after round 34: 0.02640016261752074\n",
      "Round 35\n",
      "-- Resource requested from 19 clients\n",
      "-- Filtered 19 clients\n",
      "Training loss after round 35: 0.06541985291076419\n",
      "Global model updated for round 35\n",
      "Validation loss after round 35: 0.059474184561312496\n",
      "Round 36\n",
      "-- Resource requested from 18 clients\n",
      "-- Filtered 18 clients\n",
      "Training loss after round 36: 0.05521240295763961\n",
      "Global model updated for round 36\n",
      "Validation loss after round 36: 0.051286145945002026\n",
      "Round 37\n",
      "-- Resource requested from 15 clients\n",
      "-- Filtered 15 clients\n",
      "Training loss after round 37: 0.0466486918080374\n",
      "Global model updated for round 37\n",
      "Validation loss after round 37: 0.0447085311465845\n",
      "Round 38\n",
      "-- Resource requested from 13 clients\n",
      "-- Filtered 13 clients\n",
      "Training loss after round 38: 0.04077067837765854\n",
      "Global model updated for round 38\n",
      "Validation loss after round 38: 0.03884388516471983\n",
      "Round 39\n",
      "-- Resource requested from 9 clients\n",
      "-- Filtered 9 clients\n",
      "Training loss after round 39: 0.029451399889464182\n",
      "Global model updated for round 39\n",
      "Validation loss after round 39: 0.030267842197402688\n",
      "Round 40\n",
      "-- Resource requested from 11 clients\n",
      "-- Filtered 11 clients\n",
      "Training loss after round 40: 0.03145886134480907\n",
      "Global model updated for round 40\n",
      "Validation loss after round 40: 0.03105969284879834\n",
      "Round 41\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 41: 0.024077949549186062\n",
      "Global model updated for round 41\n",
      "Validation loss after round 41: 0.02463396409773157\n",
      "Round 42\n",
      "-- Resource requested from 6 clients\n",
      "-- Filtered 6 clients\n",
      "Training loss after round 42: 0.017751755939917294\n",
      "Global model updated for round 42\n",
      "Validation loss after round 42: 0.01944408186025814\n",
      "Round 43\n",
      "-- Resource requested from 15 clients\n",
      "-- Filtered 15 clients\n",
      "Training loss after round 43: 0.041641930100085496\n",
      "Global model updated for round 43\n",
      "Validation loss after round 43: 0.040133735019532546\n",
      "Round 44\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 44: 0.019311285020265218\n",
      "Global model updated for round 44\n",
      "Validation loss after round 44: 0.021429656992466854\n",
      "Round 45\n",
      "-- Resource requested from 4 clients\n",
      "-- Filtered 4 clients\n",
      "Training loss after round 45: 0.011684155809748012\n",
      "Global model updated for round 45\n",
      "Validation loss after round 45: 0.014435836062087523\n",
      "Round 46\n",
      "-- Resource requested from 19 clients\n",
      "-- Filtered 19 clients\n",
      "Training loss after round 46: 0.05375607835652431\n",
      "Global model updated for round 46\n",
      "Validation loss after round 46: 0.04849744855805112\n",
      "Round 47\n",
      "-- Resource requested from 13 clients\n",
      "-- Filtered 13 clients\n",
      "Training loss after round 47: 0.03192816027930667\n",
      "Global model updated for round 47\n",
      "Validation loss after round 47: 0.03143396085223708\n",
      "Round 48\n",
      "-- Resource requested from 10 clients\n",
      "-- Filtered 10 clients\n",
      "Training loss after round 48: 0.022925446674311796\n",
      "Global model updated for round 48\n",
      "Validation loss after round 48: 0.023691754748656726\n",
      "Round 49\n",
      "-- Resource requested from 12 clients\n",
      "-- Filtered 12 clients\n",
      "Training loss after round 49: 0.02791364008391004\n",
      "Global model updated for round 49\n",
      "Validation loss after round 49: 0.026697070380420577\n",
      "Round 50\n",
      "-- Resource requested from 16 clients\n",
      "-- Filtered 16 clients\n",
      "Training loss after round 50: 0.03783080498012471\n",
      "Global model updated for round 50\n",
      "Validation loss after round 50: 0.035496514119877254\n",
      "Round 51\n",
      "-- Resource requested from 20 clients\n",
      "-- Filtered 20 clients\n",
      "Training loss after round 51: 0.048072801251585286\n",
      "Global model updated for round 51\n",
      "Validation loss after round 51: 0.04418545491373516\n",
      "Round 52\n",
      "-- Resource requested from 13 clients\n",
      "-- Filtered 13 clients\n",
      "Training loss after round 52: 0.026040001222898616\n",
      "Global model updated for round 52\n",
      "Validation loss after round 52: 0.026615177636692983\n",
      "Round 53\n",
      "-- Resource requested from 10 clients\n",
      "-- Filtered 10 clients\n",
      "Training loss after round 53: 0.01864451493839604\n",
      "Global model updated for round 53\n",
      "Validation loss after round 53: 0.020130141016755566\n",
      "Round 54\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 54: 0.014963002923450115\n",
      "Global model updated for round 54\n",
      "Validation loss after round 54: 0.016638370904616923\n",
      "Round 55\n",
      "-- Resource requested from 11 clients\n",
      "-- Filtered 11 clients\n",
      "Training loss after round 55: 0.019760019505258904\n",
      "Global model updated for round 55\n",
      "Validation loss after round 55: 0.02001353423107613\n",
      "Round 56\n",
      "-- Resource requested from 16 clients\n",
      "-- Filtered 16 clients\n",
      "Training loss after round 56: 0.03356054258233902\n",
      "Global model updated for round 56\n",
      "Validation loss after round 56: 0.03211163748584719\n",
      "Round 57\n",
      "-- Resource requested from 12 clients\n",
      "-- Filtered 12 clients\n",
      "Training loss after round 57: 0.020328112609557275\n",
      "Global model updated for round 57\n",
      "Validation loss after round 57: 0.02108434503653876\n",
      "Round 58\n",
      "-- Resource requested from 20 clients\n",
      "-- Filtered 20 clients\n",
      "Training loss after round 58: 0.04373784619986934\n",
      "Global model updated for round 58\n",
      "Validation loss after round 58: 0.04071058099902682\n",
      "Round 59\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 59: 0.012264995688405294\n",
      "Global model updated for round 59\n",
      "Validation loss after round 59: 0.015795096533228865\n",
      "Round 60\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 60: 0.0070209582396772155\n",
      "Global model updated for round 60\n",
      "Validation loss after round 60: 0.009702097510129938\n",
      "Round 61\n",
      "-- Resource requested from 13 clients\n",
      "-- Filtered 13 clients\n",
      "Training loss after round 61: 0.02263542232344312\n",
      "Global model updated for round 61\n",
      "Validation loss after round 61: 0.022760770454419214\n",
      "Round 62\n",
      "-- Resource requested from 11 clients\n",
      "-- Filtered 11 clients\n",
      "Training loss after round 62: 0.016294191399935484\n",
      "Global model updated for round 62\n",
      "Validation loss after round 62: 0.016587942183118272\n",
      "Round 63\n",
      "-- Resource requested from 1 clients\n",
      "-- Filtered 1 clients\n",
      "Training loss after round 63: 0.0048739739382613655\n",
      "Global model updated for round 63\n",
      "Validation loss after round 63: 0.007794170836762836\n",
      "Round 64\n",
      "-- Resource requested from 17 clients\n",
      "-- Filtered 17 clients\n",
      "Training loss after round 64: 0.03629162093772465\n",
      "Global model updated for round 64\n",
      "Validation loss after round 64: 0.03554567034134878\n",
      "Round 65\n",
      "-- Resource requested from 19 clients\n",
      "-- Filtered 19 clients\n",
      "Training loss after round 65: 0.03732089450090565\n",
      "Global model updated for round 65\n",
      "Validation loss after round 65: 0.03380522282145665\n",
      "Round 66\n",
      "-- Resource requested from 12 clients\n",
      "-- Filtered 12 clients\n",
      "Training loss after round 66: 0.016659107324718477\n",
      "Global model updated for round 66\n",
      "Validation loss after round 66: 0.018153125437367836\n",
      "Round 67\n",
      "-- Resource requested from 11 clients\n",
      "-- Filtered 11 clients\n",
      "Training loss after round 67: 0.01512748528961586\n",
      "Global model updated for round 67\n",
      "Validation loss after round 67: 0.01503634512263075\n",
      "Round 68\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 68: 0.005856544820659716\n",
      "Global model updated for round 68\n",
      "Validation loss after round 68: 0.00799793420107259\n",
      "Round 69\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 69: 0.0031821554390545417\n",
      "Global model updated for round 69\n",
      "Validation loss after round 69: 0.004190330747708989\n",
      "Round 70\n",
      "-- Resource requested from 14 clients\n",
      "-- Filtered 14 clients\n",
      "Training loss after round 70: 0.025701237081943655\n",
      "Global model updated for round 70\n",
      "Validation loss after round 70: 0.024315762728930126\n",
      "Round 71\n",
      "-- Resource requested from 12 clients\n",
      "-- Filtered 12 clients\n",
      "Training loss after round 71: 0.01538909652831838\n",
      "Global model updated for round 71\n",
      "Validation loss after round 71: 0.015838557560378424\n",
      "Round 72\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 72: 0.003305459511530999\n",
      "Global model updated for round 72\n",
      "Validation loss after round 72: 0.0056975015676774396\n",
      "Round 73\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 73: 0.0023521368091233954\n",
      "Global model updated for round 73\n",
      "Validation loss after round 73: 0.003197583832855647\n",
      "Round 74\n",
      "-- Resource requested from 2 clients\n",
      "-- Filtered 2 clients\n",
      "Training loss after round 74: 0.002047563050331923\n",
      "Global model updated for round 74\n",
      "Validation loss after round 74: 0.002713410420935058\n",
      "Round 75\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 75: 0.01145297169507276\n",
      "Global model updated for round 75\n",
      "Validation loss after round 75: 0.012420463206168884\n",
      "Round 76\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 76: 0.003582667556846799\n",
      "Global model updated for round 76\n",
      "Validation loss after round 76: 0.004761085667673291\n",
      "Round 77\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 77: 0.002471676241720733\n",
      "Global model updated for round 77\n",
      "Validation loss after round 77: 0.003392171326106311\n",
      "Round 78\n",
      "-- Resource requested from 7 clients\n",
      "-- Filtered 7 clients\n",
      "Training loss after round 78: 0.0093217441359339\n",
      "Global model updated for round 78\n",
      "Validation loss after round 78: 0.010471253311767388\n",
      "Round 79\n",
      "-- Resource requested from 3 clients\n",
      "-- Filtered 3 clients\n",
      "Training loss after round 79: 0.0023620707134304445\n",
      "Global model updated for round 79\n",
      "Validation loss after round 79: 0.003710667406267021\n",
      "Round 80\n",
      "-- Resource requested from 10 clients\n",
      "-- Filtered 10 clients\n",
      "Training loss after round 80: 0.012082281268887326\n",
      "Global model updated for round 80\n",
      "Validation loss after round 80: 0.013114202802508468\n",
      "Round 81\n",
      "-- Resource requested from 19 clients\n",
      "-- Filtered 19 clients\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This took me 2 mins to run\n",
    "Simulate Federated Learning\n",
    "A learning round consists of all clients training their local models and then aggregating the updates\n",
    "\"\"\"\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Generate a unique log directory based on the current time\n",
    "run_number = datetime.now().strftime('%m-%d_%H-%M')  # Month-Day_Hour-Minute\n",
    "log_dir = f\"./logs/{hp['run_id']}/run_{run_number}\"  # Use a timestamp to distinguish runs\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Log hyperparameters as text\n",
    "hyperparams_text = \"\\n\".join([f\"{key}: {value}\" for key, value in hp.items()])\n",
    "\n",
    "# Start time measurement\n",
    "start_time = time.time()\n",
    "\n",
    "# Federated Learning with FedCS Client Selection\n",
    "num_rounds = hp[\"num_rounds\"]\n",
    "epochs = hp[\"epochs\"]\n",
    "round_deadline = 3  # Example round deadline (in arbitrary time units)\n",
    "\n",
    "assert num_clients == len(train_loader)\n",
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "# Conduct federated learning rounds\n",
    "for round_num in range(num_rounds):\n",
    "    print(f\"Round {round_num + 1}\")\n",
    "    \n",
    "    # 2. Resource Request\n",
    "    k = random.randint(1, num_clients)\n",
    "    resource_requested_clients = select_indices(num_clients, k)\n",
    "    print(f\"-- Resource requested from {len(resource_requested_clients)} clients\")\n",
    "\n",
    "    # 2. Client Selection: Collect client updates\n",
    "    # TODO: Client ressources\n",
    "    selected_train_clients = client_selection_with_constraints([client_resources[i] for i in resource_requested_clients], round_deadline)\n",
    "    print(f\"-- Filtered {len(selected_train_clients)} clients\")\n",
    "    filtered_train_loaders = [train_loader[i] for i in selected_train_clients]\n",
    "    filtered_val_loaders = [val_loader[i] for i in selected_train_clients]\n",
    "\n",
    "    client_states = []\n",
    "    round_train_loss = 0  # Initialize round loss\n",
    "    round_val_loss = 0  # Initialize round loss\n",
    "    num_batches = 0  # Initialize batch counter\n",
    "\n",
    "    for client_train_loader, client_val_loader in zip(filtered_train_loaders, filtered_val_loaders):\n",
    "        \n",
    "        # 4. Distribution\n",
    "        client_state, client_loss = train_model(global_model, client_train_loader, hp, epochs=epochs)\n",
    "\n",
    "        # Log client loss\n",
    "        round_train_loss += client_loss\n",
    "        num_batches += 1\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss = validate_model(global_model, client_val_loader)\n",
    "        round_val_loss += val_loss\n",
    "\n",
    "        # 5. Update and Upload\n",
    "        client_states.append(client_state)\n",
    "\n",
    "    # Average loss for the round\n",
    "    avg_round_training_loss = round_train_loss / num_batches if num_batches > 0 else 0\n",
    "    writer.add_scalar(\"Metrics/Training loss\", avg_round_training_loss, round_num + 1)\n",
    "    print(f\"Training loss after round {round_num + 1}: {avg_round_training_loss}\")\n",
    "\n",
    "    # 6. Aggregation: Aggregate updates using Federated Averaging\n",
    "    new_global_state = fl.federated_averaging(global_model, client_states)  \n",
    "    global_model.load_state_dict(new_global_state)\n",
    "    print(f\"Global model updated for round {round_num + 1}\")\n",
    "\n",
    "    avg_round_val_loss = round_val_loss / num_batches if num_batches > 0 else 0\n",
    "    writer.add_scalar(\"Metrics/Validation loss\", avg_round_val_loss, round_num + 1)\n",
    "    print(f\"Validation loss after round {round_num + 1}: {avg_round_val_loss}\")\n",
    "\n",
    "# End time measurement\n",
    "end_time = time.time()\n",
    "\n",
    "# Print total execution time\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model accuracy: 96.42%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    print(f\"Global model accuracy: {test_accuracy:.2%}\")\n",
    "    return test_accuracy\n",
    "    \n",
    "# Evaluate the model on the test dataset\n",
    "test_accuracy = evaluate_model(global_model, test_loader)\n",
    "\n",
    "# End TensorBoard writer\n",
    "final_metrics = {}\n",
    "writer.add_hparams(hp, final_metrics)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global model saved at ./saved_models/global_model_beta_1732425599.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "model_save_path = f\"./saved_models/global_model_{hp['run_id']}_{run_number}.pth\"\n",
    "torch.save(global_model.state_dict(), model_save_path)\n",
    "print(f\"Global model saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

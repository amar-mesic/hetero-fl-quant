{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Conduct federated learning rounds\n",
    "for round_num in range(num_rounds):\n",
    "    print(f\"Round {round_num + 1}\")\n",
    "    \n",
    "    # 2. Resource Request\n",
    "    k = random.randint(1, num_clients)\n",
    "    resource_requested_clients = random.sample(range(num_clients), k)\n",
    "\n",
    "    # 3. Client Selection: Collect client updates\n",
    "    selected_train_clients = client_selection_with_constraints(\n",
    "        [clients[i] for i in resource_requested_clients], round_deadline\n",
    "    )\n",
    "    print(f\"-> Resource requested from {len(resource_requested_clients)} clients, {len(selected_train_clients)} clients fulfilled the criteria\")\n",
    "\n",
    "    client_states = []\n",
    "    total_loss = 0\n",
    "    num_batches = len(selected_train_clients)\n",
    "    # Parallelize client training\n",
    "    with ThreadPoolExecutor(max_workers=len(selected_train_clients)) as executor:\n",
    "        futures = {\n",
    "            executor.submit(\n",
    "                train_client_parallel, \n",
    "                client, \n",
    "                global_model, \n",
    "                epochs=1,\n",
    "                lr=lr,\n",
    "                quantize=quantize,\n",
    "                lambda_kure=lambda_kure,\n",
    "                delta=delta,\n",
    "                setup=setup\n",
    "                ): client\n",
    "            for client in selected_train_clients\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            client_id, (client_state, client_loss) = future.result()\n",
    "            # print(f\"Client {client_id} completed training with loss: {client_loss:.4f}\")\n",
    "            client_states.append(client_state)\n",
    "            total_loss += client_loss\n",
    "\n",
    "    # Compute average training loss\n",
    "    avg_round_training_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "    writer.add_scalar(\"Metrics/Training loss\", avg_round_training_loss, round_num + 1)\n",
    "    print(f\"Training loss after round {round_num + 1}: {avg_round_training_loss}\")\n",
    "\n",
    "    # 6. Aggregation: Aggregate updates using Federated Averaging\n",
    "    new_global_state = fl.federated_averaging(global_model, client_states)  \n",
    "    global_model.load_state_dict(new_global_state)\n",
    "    print(f\"Global model updated for round {round_num + 1}\")\n",
    "\n",
    "    # 7. Evaluate on Validation Set\n",
    "    val_loaders = [client.val_loader for client in clients]  # Get all validation loaders\n",
    "    val_loss, val_accuracy = validate_model(global_model, val_loaders)\n",
    "    print(f\"Validation Loss after round {round_num + 1}: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}\")\n",
    "    \n",
    "    # Optional: Log metrics for visualization\n",
    "    writer.add_scalar(\"Metrics/Validation Loss\", val_loss, round_num + 1)\n",
    "    writer.add_scalar(\"Metrics/Validation Accuracy\", val_accuracy, round_num + 1)\n",
    "\n",
    "if quantize:\n",
    "    torch.quantization.convert(global_model, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wrapper function for client training\n",
    "def train_client_parallel(client, global_model, epochs=1, lr=0.001, quantize=False, lambda_kure=0.0, delta=0.0, setup='standard'):\n",
    "    # print(f\"Training client {client.id} with resources {client.resources}\")\n",
    "    return client.id, client.train(global_model, epochs, lr, quantize, lambda_kure, delta, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import random\n",
    "import fl\n",
    "from models import *\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ClientResources:\n",
    "    speed_factor: float\n",
    "    battery_level: float\n",
    "    bandwidth: float\n",
    "    dataset_size: int\n",
    "    CPU_available: bool\n",
    "    CPU_memory_availability: float\n",
    "    GPU_available: bool\n",
    "    GPU_memory_availability: float\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Validation logic\n",
    "        if not (1 <= self.speed_factor):\n",
    "            raise ValueError(\"speed_factor must be greater than or equal to 1\")\n",
    "        if not (1 <= self.battery_level <= 100):\n",
    "            raise ValueError(\"battery_level must be between 0 and 100\")\n",
    "        if self.bandwidth < 0:\n",
    "            raise ValueError(\"bandwidth must be non-negative\")\n",
    "        if self.dataset_size <= 0:\n",
    "            raise ValueError(\"dataset_size must be positive\")\n",
    "        if not (0 <= self.CPU_memory_availability <= 128):  # Example: assuming max 128GB\n",
    "            raise ValueError(\"CPU_memory_availability must be between 0 and 128\")\n",
    "        if not (0 <= self.GPU_memory_availability <= 32):  # Example: assuming max 32GB\n",
    "            raise ValueError(\"GPU_memory_availability must be between 0 and 32\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_random(dataset_size_range=(500, 2000)):\n",
    "        \"\"\"\n",
    "        Generate random valid ClientResources.\n",
    "        \"\"\"\n",
    "\n",
    "        GPU_available = random.choice([True, False])\n",
    "\n",
    "        return ClientResources(\n",
    "            speed_factor=random.uniform(1.0, 2.0),  # Speed factor in range [0.1, 2.0]\n",
    "            battery_level=random.uniform(1, 100),   # Battery level in range [0, 100]\n",
    "            bandwidth=random.uniform(1, 100),       # Bandwidth in range [1, 100] Mbps\n",
    "            dataset_size=random.randint(*dataset_size_range),  # Dataset size\n",
    "            CPU_available=random.choice([True, False]),        # Random CPU availability\n",
    "            CPU_memory_availability=random.uniform(0, 128),    # CPU memory in GB\n",
    "            GPU_available=GPU_available,        # Random GPU availability\n",
    "            GPU_memory_availability=random.uniform(0, 32) if GPU_available else 0,  # GPU memory if available\n",
    "        )\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, id, resources: ClientResources, dataset, dataloader, val_loader):\n",
    "        \"\"\"\n",
    "        Initialize a Client object.\n",
    "\n",
    "        Parameters:\n",
    "        - id (int): The ID of the client.\n",
    "        - speed_factor (float): The speed factor of the client, which determines the training delay. It must be greater than 1\n",
    "        - dataset (torch.utils.data.Dataset): The dataset used for training.\n",
    "        - batch_size (int, optional): The batch size for the dataloader. Default is 32.\n",
    "        \"\"\"\n",
    "\n",
    "        self.id = id\n",
    "        self.resources = resources\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.dataloader = dataloader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def train(self, global_model, epochs=1, lr=0.001, quantize=False, lambda_kure=0.0, delta=0.0, setup='standard'):\n",
    "        \"\"\"\n",
    "        Train the global model on the client's local dataset using Adam optimizer.\n",
    "\n",
    "        Parameters:\n",
    "        - global_model (torch.nn.Module): The global model to be trained.\n",
    "        - epochs (int, optional): The number of training epochs. Default is 1.\n",
    "\n",
    "        Returns:\n",
    "        - state_dict (dict): The updated model parameters.\n",
    "        \"\"\"\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # track start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Directly copy the global model\n",
    "        local_model = QuantStubModel(q=quantize)\n",
    "        if(quantize):\n",
    "            local_model.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "            torch.quantization.prepare_qat(local_model, inplace=True)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        \n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(\n",
    "            local_model.parameters(), \n",
    "            lr=lr, \n",
    "            # learning hyperparameters can be set later\n",
    "            # betas=(0.9, 0.99), \n",
    "            # eps=1e-7, \n",
    "            # weight_decay=1e-4\n",
    "        )\n",
    "        \n",
    "        # Simulate training delay based on speed_factor\n",
    "        local_model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in self.dataloader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if setup == 'mqat':\n",
    "                    # Apply Pseudo-Quantization Noise (APQN)\n",
    "                    if delta is not None:\n",
    "                        for param in model.parameters():\n",
    "                            param.data = add_pseudo_quantization_noise(param, delta)\n",
    "\n",
    "                    # Apply Multi-Bit Quantization (MQAT)\n",
    "                    if bit_widths is not None:\n",
    "                        bit_width = random.choice(bit_widths)\n",
    "                        for param in model.parameters():\n",
    "                            param.data = quantize_multi_bit(param, bit_width)\n",
    "\n",
    "                outputs = local_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Kurtosis Regularization\n",
    "                if setup == 'kure':\n",
    "                    for param in local_model.parameters():\n",
    "                        loss += lambda_kure * kurtosis_regularization(param)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "\n",
    "        end_time = time.time()\n",
    "        time_elapsed = end_time - start_time\n",
    "        #print(f\"Training round complete in {time_elapsed:.2f}: seconds\")\n",
    "\n",
    "        time.sleep((self.resources.speed_factor - 1) * time_elapsed)\n",
    "        end_time = time.time()\n",
    "        time_elapsed = end_time - start_time\n",
    "        #print(f\"Client simulated to take {time_elapsed:.2f} seconds for training\")\n",
    "\n",
    "        # Return updated model parameters\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        return local_model.state_dict(), loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Number of clients and non-IID split\n",
    "train_data_size = len(mnist_train) // num_clients\n",
    "val_data_size = len(mnist_val) // num_clients\n",
    "\n",
    "if split == \"NONIID\":\n",
    "    \n",
    "    # Create indices for each class\n",
    "    train_class_indices = {i: np.where(np.array(mnist_train.targets) == i)[0] for i in range(num_classes)}\n",
    "    val_class_indices = {i: np.where(np.array(mnist_val.targets) == i)[0] for i in range(num_classes)}\n",
    "\n",
    "    train_indices = []\n",
    "    for client_id in range(num_clients):\n",
    "        chosen_classes = np.random.choice(num_classes, classes_per_client, replace=False)\n",
    "        train_client_idx = []\n",
    "        val_client_idx = []\n",
    "        for cls in chosen_classes:\n",
    "            train_cls_size = len(train_class_indices[cls]) // (num_clients // classes_per_client)\n",
    "            train_cls_idx = np.random.choice(train_class_indices[cls], train_cls_size, replace=False)\n",
    "            train_client_idx.extend(train_cls_idx)\n",
    "\n",
    "            val_cls_size = len(val_class_indices[cls]) // (num_clients // classes_per_client)\n",
    "            val_cls_idx = np.random.choice(val_class_indices[cls], val_cls_size, replace=False)\n",
    "            val_client_idx.extend(val_cls_idx)\n",
    "                        \n",
    "            # Remove assigned indices to avoid duplication\n",
    "            train_class_indices[cls] = np.setdiff1d(train_class_indices[cls], train_cls_idx)\n",
    "            val_class_indices[cls] = np.setdiff1d(val_class_indices[cls], val_cls_idx)\n",
    "\n",
    "        train_indices.append(train_client_idx)\n",
    "        val_indices.append(val_client_idx)\n",
    "\n",
    "    # Create datasets and DataLoaders for each client\n",
    "    train_datasets = [Subset(mnist_train, indices) for indices in train_indices]\n",
    "    train_loaders = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_datasets]\n",
    "    val_datasets = [Subset(mnist_val, indices) for indices in val_indices]\n",
    "    val_loaders = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in val_datasets]\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Split the training data into smaller datasets for each client\n",
    "    train_datasets = random_split(mnist_train, [train_data_size] * num_clients)\n",
    "    train_loaders = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_datasets]\n",
    "    val_datasets = random_split(mnist_train, [train_data_size] * num_clients)\n",
    "    val_loaders = [DataLoader(ds, batch_size=batch_size, shuffle=True) for ds in train_datasets]\n",
    "\n",
    "# val_loaders = DataLoader(mnist_val, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "print(f\"Simulated {num_clients} clients, each with {train_data_size} training samples, and {len(mnist_val)} validation samples\")\n",
    "# Debugging: Output distribution of classes for all clients\n",
    "for i, client_dataset in enumerate(train_datasets):\n",
    "    client_loader = torch.utils.data.DataLoader(client_dataset, batch_size=len(client_dataset))\n",
    "    client_samples, client_labels = next(iter(client_loader))\n",
    "    label_counts = Counter(client_labels.tolist())\n",
    "    print(f\"Client {i} label distribution: {dict(label_counts)}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
